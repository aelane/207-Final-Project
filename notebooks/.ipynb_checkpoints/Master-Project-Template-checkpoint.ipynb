{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Introduction \n",
    "This is the motivation and background for the project. \n",
    "\n",
    "*THIS IS THE OVERALL GOAL & DESIRED OUTCOME OF OUR PROJECT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Overview\n",
    "1. Ingest the Data\n",
    "    * Import the raw data files (from ```data/raw```)\n",
    "    * Define & call functions to perform baseline processing\n",
    "        * Categorical to dummy variables\n",
    "        * Continous numerical to mean (to 0) and variation (to 1) scaling \n",
    "        * Mapping of Target variable to appropriate classes\n",
    "    * Save data into ```data/processed``` with appropriate naming\n",
    "1. Exploratory Data Analysis\n",
    "    * This is done iteratively with Ingest Data phase above\n",
    "    * Understand the distribution of values fore features of interest\n",
    "    * Identify anomalies in the data\n",
    "    * Categories ultimately get transformed as described above for ease of interfacing with ML algorithms\n",
    "    * Pay particular attention to the output variable\n",
    "    * Consider mapping the data via latitude/longitude to visualize the mapping of the data\n",
    "    * Look into the correlation plots for features - see if there are \"duplicated\" variables\n",
    "    * Consider what kind of data features may be missing from the data set\n",
    "    * Note this strengths and limitations of the dataset\n",
    "    * Consider augmenting the dataset with additional sources\n",
    "1. Split Data into Training, Validation, & Test Sets\n",
    "    * Based on the EDA above, determine how to responsibly split the data into subsets\n",
    "    * Define functions to split up the data\n",
    "    * Save the split up data - ensure consistency with the data subsets among users\n",
    "    * Sample the data subsets to create workable subsets for faster analysis (1000s of examples instead of 100000s of examples). Call this train_mini, val_mini, test_mini\n",
    "1. KNN with routine normalization of features as baseline\n",
    "    * Load in the processed data from ```data/processed```\n",
    "    * Apply KNN for ```k = [1, 3, 5, 7, 9, 11, 13]```\n",
    "    * For each value of K, call ```classification_report``` to note various metrics\n",
    "    * Comment on the results and identify next steps and opportunities \n",
    "    * Do any follow up experiments on this initial data with KNN to explore \n",
    "1. Feature Reduction for feature relationships & visualization via PCA\n",
    "    * Apply PCA to reduce data to 2 dimensions \n",
    "    * Be sure to create callable functions\n",
    "    * Save this data as PCA data in ```data/processed```\n",
    "    * Decompose the PCA to see which features \n",
    "1. Visualize 2D PCA output\n",
    "    * Use matplotlib to visualize the training dataset\n",
    "    * Identify the key clusters - how many are there?\n",
    "    * Add coloring with the intended labels to see if there are any relationships\n",
    "1. Apply Unsupervised Learning to 2D PCA\n",
    "    * Apply K-Mean Clustering\n",
    "        * Try different numbers of clusters from 2 to 16 (or based on above) \n",
    "        * Plot the results in a grid with color coding\n",
    "    * Apply GMM Clustering\n",
    "        * Plot results\n",
    "    * Comment on the results. Draw conclusions\n",
    "1. Perform more Feature Engineering\n",
    "    * Based on insights above, fine tune the dataset to maintainable interpretability\n",
    "1. Apply Supervised Learning Methods with attention to feature weights\n",
    "    * Return to KNN - again, this is just memorizing the data, but hopefully we do better than the 1st pass\n",
    "    * Apply Decision Trees\n",
    "        * Expand to Random Forests\n",
    "        * Expand to Gradient Boosting \n",
    "    * Apply Logistic Regression\n",
    "    * For all of the above, examine the feature weights\n",
    "    * For all of the above, evaluate the interpretability\n",
    "    * For all of the above, review the performance metrics\n",
    "1. Conclusions & Recommendations\n",
    "    * What do we see in the data?\n",
    "    * Which model performs the best from a performance metrics perspective?\n",
    "    * Which model performs the best from an interpretability perspective?\n",
    "    * Which features do we identify as the strongest identifiers?\n",
    "1. Propose Next Steps\n",
    "    * If the project continued, this is what we would recommend looking at next...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To format the notebook to include plots and figures inline\n",
    "%matplotlib inline\n",
    "\n",
    "# General Python libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Feature Extraction & Unsupervised Libraries\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Learning/Model Libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Add trees libraries\n",
    "\n",
    "# Evaluation Libarires\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
